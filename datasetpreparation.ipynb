{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caefcc96-5e5b-4805-b914-91f77fcc13b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3919384048.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    ''' import os\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def convert_txt_to_csv(directory):\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.txt'):\n",
    "\n",
    "                input_file_path = os.path.join(root, filename)\n",
    "\n",
    "\n",
    "                output_csv_path = os.path.join(root, filename.replace('.txt', '.csv'))\n",
    "\n",
    "\n",
    "                with open(input_file_path, 'r') as text_file:\n",
    "\n",
    "                    lines = text_file.readlines()\n",
    "\n",
    "\n",
    "                with open(output_csv_path, 'w', newline='') as csv_file:\n",
    "\n",
    "                    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "\n",
    "                    for line in lines:\n",
    "\n",
    "                        fields = line.strip().split()\n",
    "\n",
    "\n",
    "                        csv_writer.writerow(fields)\n",
    "\n",
    "                print(f\"Conversion du fichier {input_file_path} en CSV terminée.\")\n",
    "\n",
    "\n",
    "root_directory = r\"C:\\Users\\manar.gani\\Desktop\\StressID_Dataset\\Physiological\"\n",
    "\n",
    "\n",
    "convert_txt_to_csv(root_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e81fb6-d6a2-4ebc-bb32-8b0c394c7493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def delete_txt_files(directory):\n",
    "    # Parcourir tous les fichiers et sous-répertoires dans le répertoire donné\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.txt'):\n",
    "                # Chemin complet du fichier à supprimer\n",
    "                file_path = os.path.join(root, filename)\n",
    "\n",
    "                # Supprimer le fichier\n",
    "                os.remove(file_path)\n",
    "                print(f\"Fichier {file_path} supprimé.\")\n",
    "\n",
    "# Chemin du répertoire racine à partir duquel la suppression doit commencer\n",
    "root_directory = r\"C:\\Users\\manar.gani\\Desktop\\StressID_Dataset\\Physiological\"\n",
    "\n",
    "# Appel de la fonction pour supprimer les fichiers .txt de manière récursive\n",
    "delete_txt_files(root_directory)\n",
    "\n",
    "print(\"Suppression de tous les fichiers .txt terminée.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21911a7-b08e-4e2b-811b-183bdbb93f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def delete_non_matching_csv_files(directory):\n",
    "    # Parcourir tous les fichiers et sous-répertoires dans le répertoire donné\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.csv'):\n",
    "                # Vérifier si le nom du fichier contient les mots \"ecg\", \"eda\" ou \"rr\"\n",
    "                if not any(word in filename.lower() for word in ['ecg', 'eda', 'rr']):\n",
    "                    # Chemin complet du fichier CSV à supprimer\n",
    "                    file_path = os.path.join(root, filename)\n",
    "\n",
    "                    # Supprimer le fichier\n",
    "                    os.remove(file_path)\n",
    "\n",
    "                    print(f\"Fichier {file_path} supprimé.\")\n",
    "\n",
    "# Chemin du répertoire racine à partir duquel la suppression doit commencer\n",
    "root_directory = r\"C:\\Users\\manar.gani\\Desktop\\StressID_Dataset\\Physiological\"\n",
    "\n",
    "# Appel de la fonction pour supprimer de manière récursive les fichiers CSV non correspondants\n",
    "delete_non_matching_csv_files(root_directory)\n",
    "\n",
    "print(\"Suppression des fichiers CSV non correspondants terminée.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4885246f-b0cc-4c31-9688-6a88fd9a1ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def separate_elements_into_columns(directory):\n",
    "    # Parcourir tous les fichiers et sous-répertoires dans le répertoire donné\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.csv'):\n",
    "                # Chemin complet du fichier CSV d'entrée\n",
    "                input_csv_path = os.path.join(root, filename)\n",
    "\n",
    "                # Charger le fichier CSV dans un DataFrame\n",
    "                data = pd.read_csv(input_csv_path, header=None)\n",
    "\n",
    "                # Séparer la colonne unique en trois colonnes séparées\n",
    "                data = data[0].str.split(',', expand=True)\n",
    "\n",
    "                # Renommer les colonnes\n",
    "                data.columns = ['ECG', 'EDA', 'RR']\n",
    "\n",
    "                # Supprimer la première ligne des données\n",
    "                data = data.iloc[1:]\n",
    "\n",
    "                # Chemin complet pour le fichier CSV de sortie\n",
    "                output_ecg_csv_path = os.path.join(root, f\"{os.path.splitext(filename)[0]}_ecg_data.csv\")\n",
    "                output_eda_csv_path = os.path.join(root, f\"{os.path.splitext(filename)[0]}_eda_data.csv\")\n",
    "                output_rr_csv_path = os.path.join(root, f\"{os.path.splitext(filename)[0]}_rr_data.csv\")\n",
    "\n",
    "                # Enregistrer chaque colonne dans un fichier CSV séparé\n",
    "                data['ECG'].to_csv(output_ecg_csv_path, index=False)\n",
    "                data['EDA'].to_csv(output_eda_csv_path, index=False)\n",
    "                data['RR'].to_csv(output_rr_csv_path, index=False)\n",
    "\n",
    "                print(f\"Séparation des éléments dans le fichier {input_csv_path} terminée.\")\n",
    "\n",
    "# Chemin du répertoire racine à partir duquel la séparation doit commencer\n",
    "root_directory = r\"C:\\Users\\manar.gani\\Desktop\\StressID_Dataset\\Physiological\"\n",
    "\n",
    "# Appel de la fonction pour séparer les éléments dans chaque fichier CSV de manière récursive\n",
    "separate_elements_into_columns(root_directory)\n",
    "\n",
    "print(\"Toutes les séparations d'éléments dans les fichiers CSV sont terminées.\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee16c549-8b36-4b55-a259-7e5ff4063bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def delete_non_matching_csv_files(directory):\n",
    "    # Parcourir tous les fichiers et sous-répertoires dans le répertoire donné\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.csv'):\n",
    "                # Vérifier si le nom du fichier contient les mots \"ecg\", \"eda\" ou \"rr\"\n",
    "                if not any(word in filename.lower() for word in ['ecg', 'eda', 'rr']):\n",
    "                    # Chemin complet du fichier CSV à supprimer\n",
    "                    file_path = os.path.join(root, filename)\n",
    "\n",
    "                    # Supprimer le fichier\n",
    "                    os.remove(file_path)\n",
    "\n",
    "                    print(f\"Fichier {file_path} supprimé.\")\n",
    "\n",
    "# Chemin du répertoire racine à partir duquel la suppression doit commencer\n",
    "root_directory = r\"C:\\Users\\manar.gani\\Desktop\\StressID_Dataset\\Physiological\"\n",
    "\n",
    "# Appel de la fonction pour supprimer de manière récursive les fichiers CSV non correspondants\n",
    "delete_non_matching_csv_files(root_directory)\n",
    "\n",
    "print(\"Suppression des fichiers CSV non correspondants terminée.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad397594-2e66-4a4b-810e-6a3a0cae11d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def read_eda_files(root_folder):\n",
    "    all_data = pd.DataFrame(columns=['ID', 'Task', 'EDA'])  # DataFrame to store the EDA data\n",
    "\n",
    "    # Recursive traversal of all files in the root folder\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            # Check if the file is an EDA data file\n",
    "            if file.endswith(\"_eda_data.csv\"):\n",
    "                # Extract the individual ID and task from the file name\n",
    "                file_parts = file.split('_')\n",
    "                id = file_parts[0]  # Assuming the individual ID is the first part of the file name\n",
    "                task = file_parts[1]  # Assuming the task is the second part of the file name\n",
    "\n",
    "                # Read the EDA data from the file and add it to the DataFrame\n",
    "                try:\n",
    "                    eda_data = pd.read_csv(os.path.join(root, file))\n",
    "                    eda_values = eda_data.values.flatten()\n",
    "                    df = pd.DataFrame({'ID': id, 'Task': task, 'EDA': eda_values})\n",
    "                    all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {file}: {e}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "# Root folder containing the EDA data files\n",
    "root_folder = r\"C:\\Users\\manar.gani\\Desktop\\StressID_Dataset\\Physiological\"\n",
    "print(root_folder)\n",
    "\n",
    "# Read all EDA files and store them in a DataFrame\n",
    "eda_df = read_eda_files(root_folder)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(eda_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8446fed4-c967-4440-988e-496de251136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_df.to_csv('eda_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69676ba3-d7e0-43fb-8e11-3d369f958d38",
   "metadata": {},
   "outputs": [],
   "source": [
    " import pandas as pd\n",
    "import numpy as np\n",
    "import neurokit2 as nk\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)  # Ignore the warning\n",
    "\n",
    "\n",
    "def eda_stat(array, sampling_freq=1000):\n",
    "    x = np.array(array)\n",
    "    eda = nk.eda_phasic(x, sampling_freq)\n",
    "    scr = np.array(eda['EDA_Phasic'])\n",
    "    scl = np.array(eda['EDA_Tonic'])\n",
    "    x_axis = np.linspace(0, scl.shape[0]/sampling_freq, scl.shape[0])\n",
    "    slope = np.polyfit(x_axis,scl,1)[0]\n",
    "    \n",
    "    df = pd.DataFrame(data = [x.max(), x.min(), x.mean(), x.std(),\n",
    "                              stats.kurtosis(x), stats.skew(x), np.quantile(x,0.5),\n",
    "                              x.max()/x.min(), slope, scr.max(), scr.min(), scr.mean(), scr.std(),\n",
    "                              scl.max(), scl.min(), scl.mean(), scl.std()]).T\n",
    "\n",
    "    df.columns = ['max_eda', 'min_eda', 'mean_eda', 'sd_eda', 'ku_eda', 'sk_eda', 'median_eda',\n",
    "                  'dynrange','scl_slope', 'max_scr', 'min_scr', 'mean_scr', 'sd_scr', 'max_scl', \n",
    "                  'min_scl', 'mean_scl', 'sd_scl']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def eda_time(array, sampling_freq=1000):\n",
    "    x = np.array(array)\n",
    "    eda = nk.eda_phasic(x, sampling_freq)\n",
    "    scr = np.array(eda['EDA_Phasic'])\n",
    "    \n",
    "    _, info = nk.eda_peaks(scr, sampling_freq)\n",
    "    peaks = info['SCR_Peaks']\n",
    "    amplitude = info['SCR_Amplitude']\n",
    "    recovery = info['SCR_RecoveryTime']\n",
    "    \n",
    "    nSCR = len(info['SCR_Peaks']) / (x.shape[0]/sampling_freq/60)\n",
    "    aucSCR = np.trapz(scr)\n",
    "    meanAmpSCR = np.nanmean(amplitude)\n",
    "    maxAmpSCR = np.nanmax(amplitude)\n",
    "    meanRespSCR = np.nanmean(recovery)\n",
    "    sumAmpSCR = np.nansum(amplitude) / (x.shape[0]/sampling_freq/60)\n",
    "    sumRespSCR = np.nansum(recovery) / (x.shape[0]/sampling_freq/60)\n",
    "\n",
    "    df = pd.DataFrame(data = [nSCR, aucSCR, meanAmpSCR, maxAmpSCR, meanRespSCR,\n",
    "                             sumAmpSCR, sumRespSCR]).T\n",
    "    \n",
    "    df.columns = ['nSCR', 'aucSCR', 'meanAmpSCR', 'maxAmpSCR', 'meanRespSCR',\n",
    "                  'sumAmpSCR', 'sumRespSCR']\n",
    "    return df\n",
    "\n",
    "def extract_features(eda_signal, sampling_freq=1000):\n",
    "    df_stat = eda_stat(eda_signal, sampling_freq)\n",
    "    df_time = eda_time(eda_signal, sampling_freq)\n",
    "    df = pd.concat([df_stat, df_time], axis=1)\n",
    "    return df\n",
    "\n",
    "def get_eda_features(df, sampling_freq=500):\n",
    "    groups = df.groupby(['ID', 'Task'])['EDA']  # Group by both 'ID' and 'Task'\n",
    "    features = {name: extract_features(group, sampling_freq) for name, group in groups}\n",
    "    features_df = pd.concat(features, axis=0).reset_index(level=[1, 2], drop=True)  # Reset index for both 'ID' and 'Task'\n",
    "    return features_df\n",
    "\n",
    "# Sample code to use the function\n",
    "eda_features = get_eda_features(eda_df)\n",
    "print(eda_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad38202-4a9e-4ce6-8f4a-d1c454064f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c34ee14-04a3-4080-a183-1b286574550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = eda_features.corr()\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix of EDA Features')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363b3ff2-defc-4684-a782-b4c0bb9e8812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hjorth_activity(signal):\n",
    "    activity = np.var(signal)\n",
    "    return activity\n",
    "\n",
    "def hjorth_mobility(signal):\n",
    "    derivative = np.diff(signal)\n",
    "    mobility = np.sqrt(np.var(derivative) / np.var(signal))\n",
    "    return mobility\n",
    "\n",
    "def hjorth_complexity(signal):\n",
    "    derivative = np.diff(signal)\n",
    "    mobility = hjorth_mobility(signal)\n",
    "    complexity = hjorth_mobility(derivative) / mobility\n",
    "    return complexity\n",
    "\n",
    "def extract_hjorth_features(eda_signal):\n",
    "    hjorth_act = hjorth_activity(eda_signal)\n",
    "    hjorth_mob = hjorth_mobility(eda_signal)\n",
    "    hjorth_comp = hjorth_complexity(eda_signal)\n",
    "    \n",
    "    df = pd.DataFrame(data=[hjorth_act, hjorth_mob, hjorth_comp]).T\n",
    "    df.columns = ['Hjorth_Activity', 'Hjorth_Mobility', 'Hjorth_Complexity']\n",
    "    return df\n",
    "\n",
    "def get_eda_features(df, sampling_freq=500):\n",
    "    groups = df.groupby(['ID', 'Task'])['EDA']  # Group by both 'ID' and 'Task'\n",
    "    features = {name: pd.concat([extract_features(groups, sampling_freq), extract_hjorth_features(group)], axis=1) for name, group in groups}\n",
    "    features_df = pd.concat(features, axis=0).reset_index(level=[1, 2], drop=True)  # Reset index for both 'ID' and 'Task'\n",
    "    return features_df\n",
    "\n",
    "# Sample code to use the function\n",
    "eda_features = get_eda_features(eda_df)\n",
    "print(eda_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482b51e2-7212-47fe-b50c-3d787e96ab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = eda_features.corr()\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix of EDA Features')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fb4e12-e1de-4acc-b608-e8113c0cec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read in the CSV file\n",
    "df = pd.read_csv('eda_dataset.csv')\n",
    "\n",
    "# Define a function to extract features from the EDA column\n",
    "def extract_features(series):\n",
    "    # Example features: mean, median, standard deviation, min, max\n",
    "    return {\n",
    "        'mean': series.mean(),\n",
    "        'median': series.median(),\n",
    "        'std': series.std(),\n",
    "        'min': series.min(),\n",
    "        'max': series.max()\n",
    "    }\n",
    "\n",
    "# Group the dataframe by ID and Task, and apply the feature extraction function\n",
    "grouped = df.groupby(['ID', 'Task'])['EDA'].apply(extract_features)\n",
    "\n",
    "# Convert the resulting Series to a DataFrame\n",
    "result_df = grouped.reset_index().rename(columns={'EDA': 'features'})\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d32024-f9b3-460e-b904-2d2a22e20c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df419be1-0a96-4616-8f1b-1b3651ceff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import neurokit2 as nk\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('eda_dataset.csv')\n",
    "\n",
    "# Define a function to extract relevant features from EDA values\n",
    "def extract_features(eda_values):\n",
    "    features = {}\n",
    "    features['max'] = eda_values.max()  # Extract maximum EDA value\n",
    "    # You can add more features here as needed (e.g., mean, median, etc.)\n",
    "    return features\n",
    "\n",
    "# Initialize an empty list to store extracted features\n",
    "extracted_features = []\n",
    "\n",
    "# Iterate over each unique ID and Task combination\n",
    "for (ID, Task), group in df.groupby(['ID', 'Task']):\n",
    "    # Extract EDA values for the current ID and Task\n",
    "    eda_values = group['EDA']\n",
    "    \n",
    "    # Extract relevant features from EDA values\n",
    "    features = extract_features(eda_values)\n",
    "    \n",
    "    # Add ID and Task information to the features dictionary\n",
    "    features['ID'] = ID\n",
    "    features['Task'] = Task\n",
    "    \n",
    "    # Append the features to the list\n",
    "    extracted_features.append(features)\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "extracted_features_df = pd.DataFrame(extracted_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe3efdd-405e-49de-ab8e-7c44b47731d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b4b1c4-6757-42b5-a671-a59f7dd9bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import neurokit2 as nk\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('eda_dataset.csv')\n",
    "\n",
    "# Define a function to extract relevant features from EDA values\n",
    "def extract_features(eda_values):\n",
    "    features = {}\n",
    "    # Extract relevant features directly from EDA values\n",
    "    features['max'] = eda_values.max()  # Extract maximum EDA value\n",
    "    features['mean'] = eda_values.mean()  # Extract mean EDA value\n",
    "    features['std'] = eda_values.std()  # Extract standard deviation of EDA values\n",
    "    # Add more features as needed\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Initialize an empty list to store extracted features\n",
    "extracted_features = []\n",
    "\n",
    "# Iterate over each unique ID and Task combination\n",
    "for (ID, Task), group in df.groupby(['ID', 'Task']):\n",
    "    # Extract EDA values for the current ID and Task\n",
    "    eda_values = group['EDA']\n",
    "    \n",
    "    # Extract relevant features from EDA values\n",
    "    features = extract_features(eda_values)\n",
    "    \n",
    "    # Add ID and Task information to the features dictionary\n",
    "    features['ID'] = ID\n",
    "    features['Task'] = Task\n",
    "    \n",
    "    # Append the features to the list\n",
    "    extracted_features.append(features)\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "extracted_features_df = pd.DataFrame(extracted_features)\n",
    "\n",
    "# Save the extracted features to a new CSV file\n",
    "extracted_features_df.to_csv('extracted_features.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a777f6d-f6b0-4c31-a5fd-8b395f44e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b23aa96-c0d1-4409-8377-35114fbdc083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neurokit2 as nk\n",
    "from scipy import stats\n",
    "\n",
    "def eda_stat(array, sampling_freq=1000):\n",
    "    x = np.array(array)\n",
    "    eda = nk.eda_phasic(x, sampling_freq)\n",
    "    scr = np.array(eda['EDA_Phasic'])\n",
    "    scl = np.array(eda['EDA_Tonic'])\n",
    "    x_axis = np.linspace(0, scl.shape[0]/sampling_freq, scl.shape[0])\n",
    "    slope = np.polyfit(x_axis,scl,1)[0]\n",
    "    \n",
    "    df = pd.DataFrame(data = [x.max(), x.min(), x.mean(), x.std(),\n",
    "                              stats.kurtosis(x), stats.skew(x), np.quantile(x,0.5),\n",
    "                              x.max()/x.min(), slope, scr.max(), scr.min(), scr.mean(), scr.std(),\n",
    "                              scl.max(), scl.min(), scl.mean(), scl.std()]).T\n",
    "\n",
    "    df.columns = ['max_eda', 'min_eda', 'mean_eda', 'sd_eda', 'ku_eda', 'sk_eda', 'median_eda',\n",
    "                  'dynrange','scl_slope', 'max_scr', 'min_scr', 'mean_scr', 'sd_scr', 'max_scl', \n",
    "                  'min_scl', 'mean_scl', 'sd_scl']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def eda_time(array, sampling_freq=1000):\n",
    "    x = np.array(array)\n",
    "    eda = nk.eda_phasic(x, sampling_freq)\n",
    "    scr = np.array(eda['EDA_Phasic'])\n",
    "    \n",
    "    _, info = nk.eda_peaks(scr, sampling_freq)\n",
    "    peaks = info['SCR_Peaks']\n",
    "    amplitude = info['SCR_Amplitude']\n",
    "    recovery = info['SCR_RecoveryTime']\n",
    "    \n",
    "    nSCR = len(info['SCR_Peaks']) / (x.shape[0]/sampling_freq/60)\n",
    "    aucSCR = np.trapz(scr)\n",
    "    meanAmpSCR = np.nanmean(amplitude)\n",
    "    maxAmpSCR = np.nanmax(amplitude)\n",
    "    meanRespSCR = np.nanmean(recovery)\n",
    "    sumAmpSCR = np.nansum(amplitude) / (x.shape[0]/sampling_freq/60)\n",
    "    sumRespSCR = np.nansum(recovery) / (x.shape[0]/sampling_freq/60)\n",
    "\n",
    "    df = pd.DataFrame(data = [nSCR, aucSCR, meanAmpSCR, maxAmpSCR, meanRespSCR,\n",
    "                             sumAmpSCR, sumRespSCR]).T\n",
    "    \n",
    "    df.columns = ['nSCR', 'aucSCR', 'meanAmpSCR', 'maxAmpSCR', 'meanRespSCR',\n",
    "                  'sumAmpSCR', 'sumRespSCR']\n",
    "    return df\n",
    "\n",
    "def extract_features(eda_signal, sampling_freq=1000):\n",
    "    df_stat = eda_stat(eda_signal, sampling_freq)\n",
    "    df_time = eda_time(eda_signal, sampling_freq)\n",
    "    df = pd.concat([df_stat, df_time], axis=1)\n",
    "    return df\n",
    "\n",
    "def get_eda_features(df, sampling_freq=1000):\n",
    "    groups = df.groupby(['ID', 'Task'])['EDA']\n",
    "    features = []\n",
    "    for name, group in groups:\n",
    "        features.append(extract_features(group.values, sampling_freq))\n",
    "    features_df = pd.concat(features, axis=0)\n",
    "    features_df.index = groups.apply(lambda x: x.index[0])  # Set index to first index of each group\n",
    "    return features_df\n",
    " Read your CSV file into DataFrame\n",
    "df = pd.read_csv('eda_dataset.csv')\n",
    "\n",
    "# Extract features for EDA signals\n",
    "eda_features = get_eda_features(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c914a5aa-f48a-4d0c-95ec-6ca48a74784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61ec28b-778b-446f-a8bc-fec487d13b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neurokit2 as nk\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)  # Ignore the warning\n",
    "\n",
    "\n",
    "def eda_stat(array, sampling_freq=1000):\n",
    "    x = np.array(array)\n",
    "    eda = nk.eda_phasic(x, sampling_freq)\n",
    "    scr = np.array(eda['EDA_Phasic'])\n",
    "    scl = np.array(eda['EDA_Tonic'])\n",
    "    x_axis = np.linspace(0, scl.shape[0]/sampling_freq, scl.shape[0])\n",
    "    slope = np.polyfit(x_axis,scl,1)[0]\n",
    "    \n",
    "    df = pd.DataFrame(data = [x.max(), x.min(), x.mean(), x.std(),\n",
    "                              stats.kurtosis(x), stats.skew(x), np.quantile(x,0.5),\n",
    "                              x.max()/x.min(), slope, scr.max(), scr.min(), scr.mean(), scr.std(),\n",
    "                              scl.max(), scl.min(), scl.mean(), scl.std()]).T\n",
    "\n",
    "    df.columns = ['max_eda', 'min_eda', 'mean_eda', 'sd_eda', 'ku_eda', 'sk_eda', 'median_eda',\n",
    "                  'dynrange','scl_slope', 'max_scr', 'min_scr', 'mean_scr', 'sd_scr', 'max_scl', \n",
    "                  'min_scl', 'mean_scl', 'sd_scl']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def eda_time(array, sampling_freq=1000):\n",
    "    x = np.array(array)\n",
    "    eda = nk.eda_phasic(x, sampling_freq)\n",
    "    scr = np.array(eda['EDA_Phasic'])\n",
    "    \n",
    "    _, info = nk.eda_peaks(scr, sampling_freq)\n",
    "    peaks = info['SCR_Peaks']\n",
    "    amplitude = info['SCR_Amplitude']\n",
    "    recovery = info['SCR_RecoveryTime']\n",
    "    \n",
    "    nSCR = len(info['SCR_Peaks']) / (x.shape[0]/sampling_freq/60)\n",
    "    aucSCR = np.trapz(scr)\n",
    "    meanAmpSCR = np.nanmean(amplitude)\n",
    "    maxAmpSCR = np.nanmax(amplitude)\n",
    "    meanRespSCR = np.nanmean(recovery)\n",
    "    sumAmpSCR = np.nansum(amplitude) / (x.shape[0]/sampling_freq/60)\n",
    "    sumRespSCR = np.nansum(recovery) / (x.shape[0]/sampling_freq/60)\n",
    "\n",
    "    df = pd.DataFrame(data = [nSCR, aucSCR, meanAmpSCR, maxAmpSCR, meanRespSCR,\n",
    "                             sumAmpSCR, sumRespSCR]).T\n",
    "    \n",
    "    df.columns = ['nSCR', 'aucSCR', 'meanAmpSCR', 'maxAmpSCR', 'meanRespSCR',\n",
    "                  'sumAmpSCR', 'sumRespSCR']\n",
    "    return df\n",
    "\n",
    "def extract_features(eda_signal, sampling_freq=1000):\n",
    "    df_stat = eda_stat(eda_signal, sampling_freq)\n",
    "    df_time = eda_time(eda_signal, sampling_freq)\n",
    "    df = pd.concat([df_stat, df_time], axis=1)\n",
    "    return df\n",
    "\n",
    "def get_eda_features(df, sampling_freq=500):\n",
    "    groups = df.groupby(['ID', 'Task'])['EDA']  # Group by both 'ID' and 'Task'\n",
    "    features = {name: extract_features(group, sampling_freq) for name, group in groups}\n",
    "    features_df = pd.concat(features, axis=0).reset_index()  # Reset index for both 'ID' and 'Task'\n",
    "    return features_df\n",
    "\n",
    "# Sample code to use the function\n",
    "eda_features = get_eda_features(eda_df)\n",
    "print(eda_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad7f74f-0501-4bda-8034-118f5ce88ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
